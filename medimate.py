# -*- coding: utf-8 -*-
"""Medimate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECSVNjuy3VgeETn_2dO3Yvcxf-uEjP6G
"""

!pip install python-dotenv streamlit langchain-groq
# Install Streamlit
!pip install streamlit

GROQ_API_KEY= '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'

pip install llama-index-llms-groq

from llama_index.llms.groq import Groq

llm = Groq(model="llama3-70b-8192", api_key="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")

# Call the complete method with a query
response = llm.complete("Explain the importance of low latency LLMs")

print(response)

import streamlit as st
from langchain_groq import ChatGroq
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
#GROQ_API_KEY = os.getenv("sk_ngeDqQIHYvwyCKI3HaaGWGdyb3FYHUiDLN7ulVq8GIOI5PqxhTAT")

# Initialize the model
client = ChatGroq(
  api_key = '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',
)
llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0, api_key='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')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain_groq import ChatGroq
# import os
# from dotenv import load_dotenv
# from langchain.schema import SystemMessage
# from typing import Generator
# from langchain_core.messages import SystemMessage
# from langchain.chains.conversation.memory import ConversationBufferWindowMemory
# from langchain_groq import ChatGroq
# from langchain.prompts import PromptTemplate
# from groq import Groq
# 
# st.title("Medimate")
# st.write("Hello, Welcome to Medimate.ðŸ˜Š.How can I assist you today? ")
# 
# # Initialize the model
# client = ChatGroq(
#   api_key = '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',
# )
# llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0, api_key='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')
# 
# 
# # Store LLM generated responses
# if "messages" not in st.session_state.keys():
#  st.session_state.messages = []
# 
# # Display chat messages from history on app rerun
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
# 
# # Define the system message to guide the model (health-related context)
# system_message = "You are a health assistant. Please answer only health-related questions. Do not respond to queries unrelated to health."
# 
# 
# # Sidebar configuration
# st.sidebar.title('Medimate - Preliminary Health Care Assistant')
# model = st.sidebar.selectbox(
#     'Choose a model',
#     ['llama3-8b-8192', 'mixtral-8x7b-32768', 'gemma-7b-it']
# )
# 
# def generate_chat_responses(chat_completion) -> Generator[str, None, None]:
#     """Yield chat response content from the Groq API response."""
#     for chunk in chat_completion:
#         if chunk.choices[0].delta.content:
#             yield chunk.choices[0].delta.content
# 
# query = st.chat_input("Ask anything:")
# if query:
#     # Append the user message to session state
#     st.session_state.messages.append({"role": "user", "content": query})
# 
#     # Display user message in the chat message container
#     with st.chat_message("user"):
#         st.markdown(query)
# 
#     # Construct the conversation history for context
#     conversation_history = system_message + "\n"
#     conversation_history += "\n".join(
#         f"{message['role'].capitalize()}: {message['content']}"
#         for message in st.session_state.messages
#     )
# 
#     # Call the model with the entire conversation history
#     try:
#         response_generator = llm.invoke(conversation_history)
# 
#         # Display assistant response in chat message container
#         response_content = ""
#         assistant_placeholder = st.empty()  # Placeholder for dynamic updates
# 
#         with st.chat_message("assistant"):
#             if isinstance(response_generator, Generator):
#                 # If response is a stream, iterate over chunks
#                 for chunk in response_generator:
#                     if hasattr(chunk, "content") and chunk.content:
#                         response_content += chunk.content
#                         assistant_placeholder.markdown(response_content)
#             elif hasattr(response_generator, "content"):
#                 # If response is a single object, display it all at once
#                 response_content = response_generator.content
#                 assistant_placeholder.markdown(response_content)
#             else:
#                 st.error("Unexpected response structure from the model.")
# 
#         # Add assistant response to chat history
#         if response_content:
#             st.session_state.messages.append({"role": "assistant", "content": response_content})
#         else:
#             st.error("The assistant's response was empty. Please try again.")
#     except Exception as e:
#         st.error(f"An error occurred while processing your query: {e}")
# 
# 
# 
#

!wget -q -O - ipv4.icanhazip.com
! streamlit run app.py & npx localtunnel --port 8501

def ui():
    st.markdown(
        '<link href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" rel="stylesheet">',
        unsafe_allow_html=True,
    )
    st.markdown(
        '<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" '
        'integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" '
        'crossorigin="anonymous">',
        unsafe_allow_html=True,
    )

    hide_streamlit_style = """
                <style>
                    header{visibility:hidden;}
                    .main {
                        margin-top: -20px;
                        padding-top:10px;
                    }
                    #MainMenu {visibility: hidden;}
                    footer {visibility: hidden;}
                    .reportview-container {
                        padding-top: 0;
                    }
                    .loan-summary {
                        background-color: white;
                        padding: 20px;
                        color:black;
                        border-radius: 5px;
                        box-shadow: 0 0 10px rgba(0,0,0,0.1);
                    }
                    .loan-summary h1 {
                        color: #4267B2;
                        font-size: 24px;
                        margin-bottom: 20px;
                    }
                    .loan-summary h2 {
                        color: #4267B2;
                        font-size: 18px;
                        margin-top: 15px;
                        margin-bottom: 10px;
                    }
                    .loan-summary hr {
                        margin: 15px 0;
                    }
                </style>
                """
    st.markdown(hide_streamlit_style, unsafe_allow_html=True)

    st.markdown(
        """
        <nav class="navbar fixed-top navbar-expand-lg navbar-dark" style="background-color: #4267B2;">
        <a class="navbar-brand" href="#"  target="_blank">Age and Gender Detection</a>
        </nav>
    """,
        unsafe_allow_html=True,
    )
ui()


{"role": "assistant", "content": "Hello, Welcome to Medimate.ðŸ˜Š.How can I assist you today? "}

# User input
query = st.chat_input("Ask anything:")
if query:
    try:
        # Call the model with user input
        response = llm.invoke(f"User: {query}")
        # Access the content of the response
        st.write(response.content)  # Use the .content attribute for the response
    except Exception as e:
       st.error(f"An error occurred: {e}")
 # with st.chat_message("user"):
      #  st.markdown(query)
# Accept user input
if prompt := st.chat_input("What is up?"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

query = st.chat_input("Ask anything:")
if query:
    # Append the user message to session state
    st.session_state.messages.append({"role": "user", "content": query})

    # Display user message in the chat message container
    with st.chat_message("user"):
        st.markdown(query)

    # Call the model with user input
    response = llm.invoke(f"User: {query}")

    # Check if the response is valid
    if response and hasattr(response, "content") and response.content:
        st.write(response.content)
    else:
        st.error("Unable to process your query. Please try again or ask a different question.")